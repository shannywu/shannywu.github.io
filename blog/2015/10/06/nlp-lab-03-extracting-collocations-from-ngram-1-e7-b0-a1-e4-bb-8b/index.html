<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>[NLP Lab] Extracting Collocations from ngram | learning notes</title><link rel="stylesheet" type="text/css" href="/blog/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/blog/favicon.ico"><link rel="apple-touch-icon" href="/blog/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/blog/apple-touch-icon.png"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-68820773-2','auto');ga('send','pageview');</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">[NLP Lab] Extracting Collocations from ngram</h1><a id="logo" href="/blog/.">learning notes</a><p class="description"></p></div><div id="nav-menu"><a href="/blog/." class="current"><i class="fa fa-home"> Home</i></a><a href="/blog/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/blog/about/"><i class="fa fa-user"> About</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">[NLP Lab] Extracting Collocations from ngram</h1><div class="post-meta">Oct 6, 2015<span> | </span><span class="category"><a href="/blog/categories/Lecture-Notes/">Lecture Notes</a></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Collocations-and-Collocation-Extraction"><span class="toc-number">1.</span> <span class="toc-text">Collocations and Collocation Extraction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Properties-of-Collocations"><span class="toc-number">2.</span> <span class="toc-text">Properties of Collocations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Smadja’s-Algorithm"><span class="toc-number">3.</span> <span class="toc-text">Smadja’s Algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Implementation"><span class="toc-number">4.</span> <span class="toc-text">Implementation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Slides-and-Code"><span class="toc-number">5.</span> <span class="toc-text">Slides and Code</span></a></li></ol></div></div><div class="post-content"><p>Spring 2015 Natural Language Processing Lab.<br>Week 3. and 4. Extracting Collocations from ngram<br>Instructor: Jason S. Chang</p>
<p><br><br><a id="more"></a></p>
<p>本週要根據 n-gram 來擷取 collocations。</p>
<p>那麼，什麼是 collocations 呢？</p>
<h2 id="Collocations-and-Collocation-Extraction"><a href="#Collocations-and-Collocation-Extraction" class="headerlink" title="Collocations and Collocation Extraction"></a>Collocations and Collocation Extraction</h2><p>collocation 是搭配詞。有些字天生就是和另外一些字擺在一起的，比如在中文裡我們會說「一支筆」，不會說「一條筆」；我們會說「講電話」，不會說「談電話」。</p>
<p>而英文當然也是這樣。比如說「開燈」的英文，我們會用 turn on / switch on the light，而不是 open the light。這對於非母語使用者來說，並不是一件天生就會的事，所以才需要大量閱讀，養成習慣。而 collocation extraction 就是電腦自動將這些搭配詞給擷取出來。這是講義裡的定義：</p>
<pre>
• Collocation = a pair / sequence of words co-occurring more often than chance, and conveying more meaning than the surface words, e.g., nuclear family, plastic surgery, riding boots, motor cyclist
• Collocation Extraction = Identifying collocations automatically from a corpus using a computer
</pre>

<p>為什麼 collocation 這麼重要呢？因為用對搭配詞才能將事情表得順暢、流利，並且傳達正確的意思。</p>
<h2 id="Properties-of-Collocations"><a href="#Properties-of-Collocations" class="headerlink" title="Properties of Collocations"></a>Properties of Collocations</h2><ol>
<li>Syntactic Relations<br>語法上的搭配詞，比如說形容詞後面通常會接名詞，又分為 Lexical collocations 與 Grammatical collocations。</li>
</ol>
<p><img src="https://codingdiarysite.files.wordpress.com/2015/10/screen-shot-2016-01-12-at-21-09-53.png" alt=""></p>
<ol>
<li><p>Statistical Associativity<br>根據 mutual information 來統計的相關性。比如某個形容詞 $A$ 平常少用，但他只要一出現，都是跟著某個特定的名詞 $N$ 一起出現，這樣 $A$ 和 $N$ 統計上的相關性就會很高，也可以代表他們是很適合彼此的搭配詞。</p>
</li>
<li><p>Syntactic Relation by Distance Analysis<br>假設我們今天想要知道 play 和 role 是不是合適的搭配詞，我們就去計算 play 和 role 在資料集中一起出現的次數，而我們允許這兩個字中間有其他字（但不超過特定距離）。也就是如果今天，距離 $5$ 以內的搭配詞都算數，那不管 play a role, play an important role 或是 play role 我們都算。<br><pre><br>distance and frequencies of play_role (in Google Web 1T 5gram)<br>• -4(81230) -3(161358) -2(920270) -1(255149)<br>• 4(325548) 3(3452577) 2(1428845) 1(27584)<br></pre><br>從這個統計中我們可以發現，最大值落在 play 和 role 距離 $3$ 的地方。這也就暗示了，play 和 role 中間可能有著 <strong>V. + Det. + Adj. + N.</strong> 的關係。</p>
</li>
</ol>
<h2 id="Smadja’s-Algorithm"><a href="#Smadja’s-Algorithm" class="headerlink" title="Smadja’s Algorithm"></a>Smadja’s Algorithm</h2><p>從前面的介紹可以發現：</p>
<pre>
• Collocations = word + collocate
• Collocations are relatively high frequency
• Collocations has skew distance distribution
• Peaks at some distance imply grammatical construction, e.g., AN, VN
</pre>

<p>於是有位學者就提出了一套擷取搭配詞的演算法。</p>
<p>他先去計算字 (base word) 和搭配詞 (collocate) 之間的距離 (distance)，接著他列出了三個條件，只要符合這三個條件的搭配詞組，就是這個 base word 合適的搭配詞。這三個條件分別是：</p>
<pre>
(C1) Count(base_word, collocate) &gt; f(average) + 1σ (standard deviation)
(C2) Count(base_word, collocate, distance) spread out non-uniformly (has 1-2 peaks)
(C3) Some distances where Count(base_word, collocate, distance) peaks
</pre>

<p>更數學一點的表示方法如下：<br><img src="https://codingdiarysite.files.wordpress.com/2015/10/screen-shot-2016-01-12-at-21-38-53.png" alt=""></p>
<p>舉個例子：<br><img src="https://codingdiarysite.files.wordpress.com/2015/10/screen-shot-2016-01-12-at-21-39-50.png" alt=""></p>
<p>也就是說不論是平均出現頻率、平均平方差等統計上的數據，都必須大於特定數字，才能成為合適的搭配詞組。</p>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><ol>
<li><p>Generate ngrams for a given corpus $(n = 2-5)$</p>
<p>首先，把資料集的所有句子都讀入，並產生 n-gram，同時計算每個 n-gram 出現過幾次。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ngram_counts = defaultdict(Counter)</div><div class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'sents.txt.50000'</span>) <span class="keyword">as</span> text_file:</div><div class="line">    <span class="keyword">for</span> index,<span class="built_in">line</span> <span class="keyword">in</span> enumerate(text_file):</div><div class="line">        <span class="keyword">words</span> = wordpunct_tokenize(<span class="built_in">line</span>)</div><div class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">2</span>, max_distance + <span class="number">2</span>):</div><div class="line">            ngram_counts[n].update(<span class="built_in">filter</span>(ngram_is_valid, to_ngrams(<span class="keyword">words</span>, n)))</div></pre></td></tr></table></figure>
<p><code>wordpunct_tokenize()</code> 是用來切字的。而這裡的 $max_distance = 5$，代表搭配詞與 base word 的最大距離。<code>to_grams()</code> 則是用 <code>izip</code> 將每個單字合起來成 n-gram，詳細程式碼如下：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_ngrams</span><span class="params">(unigrams, length)</span></span><span class="symbol">:</span></div><div class="line">    <span class="keyword">return</span> izip(*[unigrams[<span class="symbol">i:</span>] <span class="keyword">for</span> i <span class="keyword">in</span> range(length)])</div></pre></td></tr></table></figure>
<p>至於 <code>ngram_is_valid()</code> 是去檢查這個 n-gram 裡面有沒有含數字或是符號，或者頭尾是 stopwords 的，都把他們過濾掉。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">eng_stopwords = <span class="keyword">set</span>(stopwords.words(<span class="string">'english'</span>))</div><div class="line">eng_symbols = <span class="string">'&#123;&#125;&amp;amp;quot;\'()[].,:;+!?-*/&amp;amp;amp;|&amp;amp;lt;&amp;amp;gt;=~$'</span></div><div class="line"><span class="keyword">def</span> ngram_is_valid(ngram):</div><div class="line">    <span class="keyword">first</span>, <span class="keyword">last</span> = ngram[<span class="number">0</span>], ngram[<span class="number">-1</span>]</div><div class="line">    <span class="keyword">if</span> <span class="keyword">first</span> <span class="keyword">in</span> eng_stopwords <span class="keyword">or</span> <span class="keyword">last</span> <span class="keyword">in</span> eng_stopwords: <span class="keyword">return</span> <span class="literal">False</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">any</span>( <span class="keyword">num</span> <span class="keyword">in</span> <span class="keyword">first</span> <span class="keyword">or</span> <span class="keyword">num</span> <span class="keyword">in</span> <span class="keyword">last</span> <span class="keyword">for</span> <span class="keyword">num</span> <span class="keyword">in</span> <span class="string">'0123456789'</span>): <span class="keyword">return</span> <span class="literal">False</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">any</span>( eng_symbol <span class="keyword">in</span> word <span class="keyword">for</span> word <span class="keyword">in</span> ngram <span class="keyword">for</span> eng_symbol <span class="keyword">in</span> eng_symbols): <span class="keyword">return</span> <span class="literal">False</span></div><div class="line">    <span class="keyword">return</span> <span class="literal">True</span></div></pre></td></tr></table></figure>
<p>整個建構完之後，<code>ngram_counts</code> 會長這樣（範例只有三個句子）：</p>
<p><img src="https://codingdiarysite.files.wordpress.com/2016/01/screen-shot-2016-01-12-at-23-07-02.png" alt=""><br><code>key</code> 是 ngram 長度(2-6)，<code>value</code> 則是一個 Counter，儲存 ngram 及他們出現的次數。比如長度為 $3$ 的 ‘plat the role’ 就出現 2 次。</p>
</li>
<li><p>Generate skip bigrams from ngrams $(-5 \leq d \leq 5)$<br>skip bigrams 的概念是，任兩個字中間有其他字，只要那兩個字的距離不大於 $5$，都算是 bigram。<br>這部分針對每個在 <code>ngram_counts[n]</code>（這是一個 Counter）key 裡的 ngram，不管 ngram 的長度為何，都去計算它的 skip bigram。</p>
<figure class="highlight prolog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">skip_bigram_info = defaultdict(partial(defaultdict, <span class="symbol">Counter</span>))</div><div class="line">for n in range(<span class="number">2</span>, max_distance + <span class="number">2</span>):</div><div class="line">    for ngram in ngram_counts[n].keys():</div><div class="line">        skip_bigram_info[ngram[<span class="number">0</span>]][ngram[<span class="number">-1</span>]][n<span class="number">-1</span>] += ngram_counts[n][ngram]</div><div class="line">        skip_bigram_info[ngram[<span class="number">-1</span>]][ngram[<span class="number">0</span>]][<span class="number">1</span>-n] += ngram_counts[n][ngram]</div><div class="line">print len(skip_bigram_info[<span class="string">'play'</span>])</div></pre></td></tr></table></figure>
<p>至於 <code>skip_bigram_info[ngram[0]][ngram[-1]][n-1] += ngram_counts[n][ngram]</code> 是什麼？（雖然我覺得就算解釋了我下次回來看還是會想不起來…）</p>
<p><code>skip_bigram_info</code> 是一個 defaultdict，它的 key 是 base word，而 value 就有點複雜，是一個長這樣的東西：(defaultdict, Counter)。這是什麼呢？簡單來講就是一個 dict，而這個 dict 的 value 是一個 Counter 結構。而這個 Counter 又有自己的 key 和 value，分別是距離和出現頻率。來個圖解：</p>
<p><img src="https://codingdiarysite.files.wordpress.com/2016/01/screen-shot-2016-01-12-at-23-09-15.png" alt=""></p>
<p>play 是 <code>skip_bigram_info</code> 的 key，後面接的那一串則是 value。而這個 value 本身是一個 defaultdict，key 是 collocate，value 是一個 Counter。這個 Counter 又有 key 和 value 分別是 word 和 collocate 的距離，還有出現的頻率。好，繞口令結束。（我真的不期待一個禮拜後的我能看得懂囧）</p>
<p>好，現在我們回到 <code>skip_bigram_info[ngram[0]][ngram[-1]][n-1]</code>，假設 ngram 叫做 “play the role”，那麼 <code>ngram[0]</code> = ‘play’，<code>ngram[-1]</code> = ‘role’，$n-1$ 則是 play 和 role 之間的距離。好，到這裡會發現東西都差不多有了，只剩下頻率了。也就是 Counter 裡面的那個 value。這東西怎麼算呢？當然就要用到剛剛算好的 <code>ngram_counts</code> 了，所以 <code>ngram_counts[n][ngram]</code> 出現了！</p>
<p>他來做什麼？記不記得剛剛 <code>ngram_counts</code> 長什麼樣子？ <code>ngram_counts[n][ngram]</code> 是一個數值，也就是頻率，是 ngram 出現的頻率，而 <code>n</code> 在這邊是 n-gram 的長度。所以假設 $n=3$, $ngram$ = ‘play the role’，<code>ngram_counts[3][&#39;play the role&#39;]</code> 就是 $2$，代表 ‘play the role’ 出現過 2 次。</p>
<p>那這個數值又跟 <code>skip_bigram_info[&#39;play&#39;][&#39;role&#39;][n-1]</code> 有什麼關係呢？假設 play 和 role 的距離為 $2$，也就是 $n=3$，<code>ngram_counts[3][&#39;play the role&#39;]</code> 就會傳回 $2$。意思就是 play 和 role 距離為 $2$ 的 n-gram 出現 2 次。</p>
<p>那 play 和 role 有沒有可能距離為 $3$ 或 $4$？當然有。所以 <code>skip_bigram_info[&#39;play&#39;][&#39;role&#39;]</code> 這個 Counter 裡就會有各種距離(key)，從 $-5$ ~ $5$，以及他們出現的頻率。（註：距離為負值，代表 collocate 出現在 base word 的前面。）</p>
<p>沒想到短短六行程式碼，我需要花這的多篇幅解釋，真心覺得我能寫完這段真的很了不起。（呼～）</p>
</li>
<li><p>Generate average frequency and standard deviation of a word (e.g., play) with all<br>its collocate</p>
<p>接下來，要計算平均頻率與其他各種統計資訊了。</p>
<figure class="highlight prolog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">skip_bigram_abc = defaultdict(lambda:defaultdict(lambda:<span class="number">0.0</span>))</div><div class="line">for baseWord in skip_bigram_info.keys():</div><div class="line">    avg_f = <span class="number">0</span></div><div class="line">    dev = <span class="number">0</span></div><div class="line">    for colWord in skip_bigram_info[baseWord].keys():</div><div class="line">        skip_bigram_abc[(baseWord, colWord, <span class="string">'freq'</span>)] = sum(skip_bigram_info[baseWord][colWord].values())</div><div class="line">        mean = skip_bigram_abc[(baseWord, colWord, <span class="string">'freq'</span>)]/<span class="number">10</span></div><div class="line">        spread = <span class="number">0</span></div><div class="line">        for i in range(<span class="number">-5</span>,<span class="number">0</span>) + range(<span class="number">1</span>,<span class="number">6</span>):</div><div class="line">            spread += ((skip_bigram_info[baseWord][colWord][i] - mean)**<span class="number">2</span>)</div><div class="line">        skip_bigram_abc[(baseWord, colWord, <span class="string">'spread'</span>)] = spread/<span class="number">10</span></div><div class="line">        avg_f += skip_bigram_abc[(baseWord, colWord, <span class="string">'freq'</span>)]</div><div class="line">    skip_bigram_abc[(baseWord, <span class="string">'avg_freq'</span>)] = avg_f/len(skip_bigram_info[baseWord])</div><div class="line"></div><div class="line">    for colWord in skip_bigram_info[baseWord].keys():</div><div class="line">        dev += (skip_bigram_abc[(baseWord, colWord, <span class="string">'freq'</span>)] - skip_bigram_abc[(baseWord,<span class="string">'avg_freq'</span>)])**<span class="number">2</span></div><div class="line">    skip_bigram_abc[(baseWord, <span class="string">'dev'</span>)] = math.sqrt(float(dev) / float(len(skip_bigram_info[baseWord])))</div></pre></td></tr></table></figure>
<p>這裡用 <code>skip_bigram_abc</code> 來儲存相關的統計資訊。</p>
<ul>
<li><code>(baseWord, colWord, &#39;freq&#39;)</code> 是出現的總次數，計算的方法是把 <code>skip_bigram_info[baseWord][colWord]</code> 的所有 value 加總</li>
<li><code>mean</code> 是除以 $U_{0}$ 之後的結果 ($U_{0} = 10$)</li>
<li><code>(baseWord, colWord, &#39;spread&#39;)</code> 是 base 和 col 的距離從 $-5$ ~ $5$ 的均方差的總和再除以 $U_{0}$</li>
<li><code>(baseWord, &#39;avg_freq&#39;)</code> 是平均頻率，也就是頻率總和除以 baseword 總數</li>
<li><code>(baseWord, &#39;dev&#39;)</code> 則是標準差<br>都統計完之後，就來看看 base word 和 collocate 之間的距離分布吧。這邊用 <a href="http://pandas.pydata.org/" target="_blank" rel="external">pandas</a> 來畫出 play 和 role 的距離分布圖。</li>
</ul>
<figure class="highlight prolog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import pandas</div><div class="line"><span class="comment">%matplotlib inline</span></div><div class="line">play_role_distances_count = pandas.<span class="symbol">Series</span>(skip_bigram_info[<span class="string">'play'</span>][<span class="string">'role'</span>].values(), index= skip_bigram_info[<span class="string">'play'</span>][<span class="string">'role'</span>].keys()).sort_index()</div><div class="line">play_role_distances_count.plot(kind=<span class="string">'bar'</span>)</div></pre></td></tr></table></figure>
<p>畫出來的結果長這樣：<br><img src="https://codingdiarysite.files.wordpress.com/2015/10/screen-shot-2016-01-14-at-16-06-08.png" alt=""></p>
</li>
<li><p>Check $C_{1}$, $C_{2}$ and $C_{3}$</p>
<p>以上都統計完之後，就要進入最後階段啦！！</p>
<p>首先，用條件 $C_{1}$ 來過濾掉 weak collocates。</p>
<figure class="highlight prolog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">if skip_bigram_abc[(baseWord, <span class="string">'dev'</span>)] == <span class="number">0</span>:</div><div class="line">    strength = <span class="number">0</span></div><div class="line">else:</div><div class="line">    strength = (skip_bigram_abc[(baseWord, colWord, <span class="string">'freq'</span>)] - skip_bigram_abc[(baseWord,<span class="string">'avg_freq'</span>)]) / float(skip_bigram_abc[(baseWord, <span class="string">'dev'</span>)])</div></pre></td></tr></table></figure>
<p>接著，對每個 skip bigram，利用條件 $C_{2}$ 來看看分佈情形、條件 $C_{3}$ 來找峰值。($k_{0} = 1, U_{0} = 10$)</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># C2</span></div><div class="line">spread = skip_bigram_abc[(baseWord, colWord, <span class="string">'spread'</span>)]</div><div class="line"></div><div class="line"><span class="comment"># C3</span></div><div class="line"><span class="keyword">for</span> i in range(-<span class="number">5</span>,<span class="number">0</span>) + range(<span class="number">1</span>,<span class="number">6</span>):</div><div class="line">    peak = skip_bigram_abc[(baseWord, colWord, <span class="string">'freq'</span>)]/<span class="number">10</span> + (k1 * math.sqrt(skip_bigram_abc[(baseWord, colWord, <span class="string">'spread'</span>)]))</div><div class="line">    p = skip_bigram_info[baseWord][colWord][i]</div><div class="line"></div><div class="line">    <span class="keyword">if</span> strength &amp;amp;amp;amp;<span class="keyword">gt</span>;= k<span class="number">0</span> <span class="keyword">and</span> spread &amp;amp;amp;amp;<span class="keyword">gt</span>;= U<span class="number">0</span> <span class="keyword">and</span> p &amp;amp;amp;amp;<span class="keyword">gt</span>;= peak:</div><div class="line">        collocations.append([baseWord, colWord, i, strength, spread, peak, p]) <span class="comment"># i = distance</span></div></pre></td></tr></table></figure>
<p>最後，將這些沒有被淘汰掉的 collocations 輸出，就大功告成了！！！<br>不過在這之前，pandas 也想再出來秀一下，這次登場的是 Dataframe。這邊是要列出所有 collocation Dataframe。</p>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># cc = [(<span class="string">'base word'</span>, <span class="string">'collocate'</span>, <span class="string">'distance'</span>, <span class="string">'strength'</span>, <span class="string">'spread'</span>, <span class="string">'peak'</span>, <span class="string">'p'</span>).......]</div><div class="line"><span class="keyword">import</span> pandas</div><div class="line">collocations = smadjas_skip_bigram()</div><div class="line">print collocations[<span class="number">0</span>]</div><div class="line">collocations_df = pandas.DataFrame(collocations,</div><div class="line">                                   columns = [<span class="string">'base word'</span>, <span class="string">'collocate'</span>, <span class="string">'distance'</span>, <span class="string">'strength'</span>, <span class="string">'spread'</span>, <span class="string">'peak'</span>, <span class="string">'p'</span>])</div><div class="line">collocations_df = collocations_df.set_index([<span class="string">'base word'</span>, <span class="string">'collocate'</span>, <span class="string">'distance'</span>]).sort_index()</div></pre></td></tr></table></figure>
<p>如此一來，就可以很清楚的看到各個 base word 跟 collocate 的距離、strength、spread、peak 等等的關係。以下是以 work 為 base word，與各個 collocate 的資料呈現。<br><img src="https://codingdiarysite.files.wordpress.com/2015/10/screen-shot-2016-01-14-at-16-12-33.png" alt=""></p>
</li>
</ol>
<p>怎麼比前幾次完成都還要激動…應該的吧，看我這次這麼認真寫了這麼多，下次這樣寫不知道是什麼時候了哈哈哈。</p>
<p>對了，這次有用到一個玩具，叫做 ipython notebook，可以用來呈現一些關於這個 lab 的視覺化資料。先放個<a href="http://mindonmind.github.io/2013/02/08/ipython-notebook-interactive-computing-new-era/" target="_blank" rel="external">參考連結</a>。</p>
<h2 id="Slides-and-Code"><a href="#Slides-and-Code" class="headerlink" title="Slides and Code"></a>Slides and Code</h2><ul>
<li><a href="https://github.com/NLPCourse-2017/week-04-collocation-extraction" target="_blank" rel="external">NLP Lab. 2017 Week 04 github repo</a></li>
</ul>
</div><div class="tags"><a href="/blog/tags/collocation/">collocation</a></div><div class="post-nav"><a href="/blog/2015/10/20/nlp-lab-05-selecting-good-dictionary-examples-gdex/" class="pre">[NLP Lab] Selecting Good Dictionary Examples, GDEX</a><a href="/blog/2015/09/22/nlp-lab-02-spell-checker-using-web-corpus-e7-b0-a1-e4-bb-8b-e8-88-87-e5-af-a6-e4-bd-9c/" class="next">[NLP Lab] Spell Checker Using Web Corpus</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://shannywu.github.io/blog"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/blog/2020/04/19/airflow-kubernetes-pod-operator-cloud-composer/">[Airflow] Using the KubernetesPodOperator on Cloud Composer</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2020/04/03/airflow-scheduling/">[Airflow] Scheduling</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2019/02/16/spark-job-on-amazon-emr/">[Spark] Run Spark Job on Amazon EMR</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/11/22/python-nltk-tools/">[Python] NLTK 工具整理</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/07/26/windows-flask-mod-wsgi-apache-on-windows/">Flask + mod_wsgi + Apache on Windows</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/07/17/python-nltk-wordnet/">[Python] NLTK and WordNet</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/06/16/tools-tmux-a-terminal-multiplexer/">tmux - A terminal multiplexer</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/05/24/mda-ch9-recommendation-systems/">[Massive Data Analysis] Recommendation Systems</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/05/24/mda-ch8-advertising-on-the-web/">[Massive Data Analysis] Advertising on the Web</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/05/23/mda-chapter-7-clustering/">[Massive Data Analysis] Clustering</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Data-Engineering/">Data Engineering</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Lecture-Notes/">Lecture Notes</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Miscellaneous/">Miscellaneous</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/blog/tags/flask/" style="font-size: 15px;">flask</a> <a href="/blog/tags/nltk/" style="font-size: 15px;">nltk</a> <a href="/blog/tags/re/" style="font-size: 15px;">re</a> <a href="/blog/tags/standford-parser/" style="font-size: 15px;">standford parser</a> <a href="/blog/tags/wordnet/" style="font-size: 15px;">wordnet</a> <a href="/blog/tags/linux/" style="font-size: 15px;">linux</a> <a href="/blog/tags/sort/" style="font-size: 15px;">sort</a> <a href="/blog/tags/grep/" style="font-size: 15px;">grep</a> <a href="/blog/tags/zgrep/" style="font-size: 15px;">zgrep</a> <a href="/blog/tags/edit-distance/" style="font-size: 15px;">edit distance</a> <a href="/blog/tags/spelling-correction/" style="font-size: 15px;">spelling correction</a> <a href="/blog/tags/collocation/" style="font-size: 15px;">collocation</a> <a href="/blog/tags/Linggle/" style="font-size: 15px;">Linggle</a> <a href="/blog/tags/map-reduce/" style="font-size: 15px;">map reduce</a> <a href="/blog/tags/pattern-grammar/" style="font-size: 15px;">pattern grammar</a> <a href="/blog/tags/WriteAhead/" style="font-size: 15px;">WriteAhead</a> <a href="/blog/tags/filter/" style="font-size: 15px;">filter</a> <a href="/blog/tags/lambda/" style="font-size: 15px;">lambda</a> <a href="/blog/tags/map/" style="font-size: 15px;">map</a> <a href="/blog/tags/match/" style="font-size: 15px;">match</a> <a href="/blog/tags/search/" style="font-size: 15px;">search</a> <a href="/blog/tags/reduce/" style="font-size: 15px;">reduce</a> <a href="/blog/tags/tmux/" style="font-size: 15px;">tmux</a> <a href="/blog/tags/apache/" style="font-size: 15px;">apache</a> <a href="/blog/tags/mod-wsgi/" style="font-size: 15px;">mod_wsgi</a> <a href="/blog/tags/windows/" style="font-size: 15px;">windows</a> <a href="/blog/tags/Airflow/" style="font-size: 15px;">Airflow</a> <a href="/blog/tags/Data-Engineering/" style="font-size: 15px;">Data Engineering</a> <a href="/blog/tags/Cloud-Composer/" style="font-size: 15px;">Cloud Composer</a> <a href="/blog/tags/GCP/" style="font-size: 15px;">GCP</a> <a href="/blog/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/blog/tags/EMR/" style="font-size: 15px;">EMR</a> <a href="/blog/tags/AWS/" style="font-size: 15px;">AWS</a></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/blog/." rel="nofollow">learning notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/blog/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/blog/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/blog/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/blog/js/smartresize.js?v=0.0.0"></script></div></body></html>