<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>[Massive Data Analysis] Clustering | learning notes</title><link rel="stylesheet" type="text/css" href="/blog/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/blog/favicon.ico"><link rel="apple-touch-icon" href="/blog/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/blog/apple-touch-icon.png"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-68820773-2','auto');ga('send','pageview');</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">[Massive Data Analysis] Clustering</h1><a id="logo" href="/blog/.">learning notes</a><p class="description"></p></div><div id="nav-menu"><a href="/blog/." class="current"><i class="fa fa-home"> Home</i></a><a href="/blog/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/blog/about/"><i class="fa fa-user"> About</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">[Massive Data Analysis] Clustering</h1><div class="post-meta">May 23, 2016<span> | </span><span class="category"><a href="/blog/categories/Lecture-Notes/">Lecture Notes</a></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Clustering"><span class="toc-number">1.</span> <span class="toc-text">Clustering</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Curse-of-dimensionality"><span class="toc-number">2.</span> <span class="toc-text">Curse of dimensionality</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cosine-Jaccard-and-Euclidean-Distance"><span class="toc-number">3.</span> <span class="toc-text">Cosine, Jaccard, and Euclidean Distance</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Methods-of-Clustering"><span class="toc-number">4.</span> <span class="toc-text">Methods of Clustering</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hierarchical-Clustering"><span class="toc-number">5.</span> <span class="toc-text">Hierarchical Clustering</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#K-means-Clustering"><span class="toc-number">6.</span> <span class="toc-text">K-means Clustering</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BFR-Algorithm"><span class="toc-number">7.</span> <span class="toc-text">BFR Algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Slides"><span class="toc-number">8.</span> <span class="toc-text">Slides</span></a></li></ol></div></div><div class="post-content"><p>Spring 2016 Massive Data Analysis Lecture Notes<br>Ch7. Clustering<br>Instructor: Jia-Shung Wang</p>
<p><br><br><a id="more"></a></p>
<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><ul>
<li>簡介：將點分群 Given a <strong>set of points</strong>, with a notion of <strong>distance</strong> between points, <strong>group the points</strong> into some number of <strong><em>clusters</em></strong>, so that <ul>
<li>Members of a cluster are close/similar to each other 群內的點相似度越高越好</li>
<li>Members of different clusters are dissimilar 群跟群之間相似度越低越好</li>
<li>而通常：Points are in a high-dimensional space, Similarity is defined using a distance measure</li>
</ul>
</li>
</ul>
<h2 id="Curse-of-dimensionality"><a href="#Curse-of-dimensionality" class="headerlink" title="Curse of dimensionality"></a>Curse of dimensionality</h2><ul>
<li>簡介：clustering in high-dimensional spaces is difficult. 當資料的維度高時，做clustering 會變得困難</li>
<li>為什麼重要（來自上學期 Data Mining 講義）：When dimensionality increase, data becomes increasingly<strong> sparse </strong>in the space that it occupies. High dimensional data is difficult to work with since <strong>adding more features can increase noise</strong>. Also, it results in <strong>increasing running time</strong>, <strong>reducing accuracy and increasing complexity. </strong></li>
<li>特性：<ul>
<li>Almost all pairs of points are equally far away from one another. 就是前面為什麼重要裡說的 sparse<ul>
<li>如果資料是一維，那麼這些點只會在線上，假設這些點都介於 0-1 之間，那麼任意兩點的距離平均是 1/3；如果維度很高，點會分佈在一個很高維的空間，任兩點的距離自然就遠了</li>
<li>如果沒有任兩點是接近的，那麼就很難做 clustering 了</li>
<li>The upper limit between two points is $\sqrt{d}$. In fact, almost all points will have a distance close to the average distance. (Almost all pairs of points are at about the same distance) 基本上任兩點的距離都會接近平均距離，但是如果任意兩點的距離都相當（都大約等於平均距離）那也沒辦法做出有效的 clustering</li>
</ul>
</li>
<li>Almost any two vectors are almost orthogonal. 任兩個向量（也就是任三個點）的角度會近似直角<ul>
<li>但是如果維度小，這就不成立</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Cosine-Jaccard-and-Euclidean-Distance"><a href="#Cosine-Jaccard-and-Euclidean-Distance" class="headerlink" title="Cosine, Jaccard, and Euclidean Distance"></a>Cosine, Jaccard, and Euclidean Distance</h2><ul>
<li><strong>Sets as vectors:</strong> Measure similarity by the <strong>cosine distance</strong></li>
<li><strong>Sets as sets:</strong> Measure similarity by the <strong>Jaccard distance</strong></li>
<li><strong>Sets as points:</strong> Measure similarity by <strong>Euclidean distance</strong></li>
</ul>
<h2 id="Methods-of-Clustering"><a href="#Methods-of-Clustering" class="headerlink" title="Methods of Clustering"></a>Methods of Clustering</h2><ul>
<li>Hierarchical (see V.)<ul>
<li><strong>Agglomerative</strong> (bottom up)<ul>
<li>Initially, each point is a cluster</li>
<li>Repeatedly combine the two “nearest” clusters into one</li>
</ul>
</li>
<li><strong>Divisive</strong> (top down)：Start with one cluster and recursively split it</li>
</ul>
</li>
<li>Point assignment<ul>
<li>Maintain a set of clusters</li>
<li>Points belong to “nearest” cluster    </li>
</ul>
</li>
</ul>
<h2 id="Hierarchical-Clustering"><a href="#Hierarchical-Clustering" class="headerlink" title="Hierarchical Clustering"></a>Hierarchical Clustering</h2><ul>
<li><p><strong>Agglomerative</strong>：Repeatedly combine two nearest clusters</p>
<ul>
<li><p>三個重要問題</p>
<ol>
<li><p>How to represent a cluster of many points?</p>
<ul>
<li>Euclidean case: each cluster has a <em>centroid</em> = average of its (data) points 群內每個資料點的平均，centroid 不一定是一個資料點，而是一個“人造的”位置</li>
<li>Non-Euclidean case: <em>clustroid</em>  = (data) point “<strong>closest</strong>” to other points 在現存的資料點中，離群內每個資料點最近的那個點稱為 clustroid 而 “closest” 的可能意思有：<ul>
<li>Smallest maximum distance to other points</li>
<li>Smallest average distance to other points</li>
<li>Smallest sum of squares of distances to other points<br><img src="https://hackpad-attachments.imgix.net/hackpad.com_5E9hkKwu42g_p.537642_1463758490812_undefined?fit=max&amp;w=882" alt=""></li>
</ul>
</li>
</ul>
</li>
<li><p>How to determine <strong>“nearness”</strong> of clusters?</p>
<ul>
<li>Euclidean case: Measure cluster distances by distances of centroids</li>
<li>Non-Euclidean case:<ul>
<li>Step 1: Treat clustroid as if it were centroid, when computing inter-cluster distances</li>
<li>Step 2: Intercluster distance = minimum of the distances between any two points, one from each cluster</li>
<li>Step 3: Pick a notion of “<strong>cohesion</strong>” of clusters, and merge clusters whose <strong>union</strong> is most cohesive 耦合度，選擇耦合度最高的，至於耦合度怎麼看：<ul>
<li>Use the <strong>diameter</strong> of the merged cluster = maximum distance between points in the cluster 相隔最遠的兩點的距離</li>
<li>Use the <strong>average distance</strong> between points in the cluster 點與點間距離的平均</li>
<li>Use a density-based approach. Take the diameter or avg. distance, e.g., and divide by the number of points in the cluster 群內密度</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>When to stop combining clusters? </li>
</ol>
</li>
<li>Implementation and Efficiency<ul>
<li>At each step, compute pairwise distances between all pairs of clusters, then merge<ul>
<li>$O(n^{3})$</li>
<li>每一步都計算任兩點間的距離，目的是找到 the best merger（也就是距離最短的兩群）</li>
<li>The initial step takes $O(n^{2})$ time, but subsequent steps take time proportional to $(n − 1)^{2}$, $(n − 2)^{2}$, the sum of squares up to $n$ is $O(n^{3})$    </li>
</ul>
</li>
<li>較有效率的方法：Using priority queue can reduce time<ul>
<li>$O(n^{2}logn)$ 但還是太貴</li>
<li>一開始就算好兩點間的距離</li>
<li>將任兩點（一個 pair）與他們間的距離存進 priority queue，這樣掃一遍就可以找到最小距離 —&gt; $O(n^{2})$</li>
<li>當我們決定要 merge 某兩個群 C 和 D 的同時，移除所有包含這兩群的 pair。因為最多要進行 $2n$ 次移除，而 priority queue deletion 需要 $O(logn)$ 所以總共需要 —&gt; $O(nlogn)$</li>
<li>計算新的群和其他群（沒有被刪掉的那些）的距離。最多 _ 個 entry 要被插入 queue，而 priority queue 的插入需要 $O(logn)$ —&gt; $O(nlogn)$ </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="K-means-Clustering"><a href="#K-means-Clustering" class="headerlink" title="K-means Clustering"></a>K-means Clustering</h2><ul>
<li>簡介：分群演算法，必須事前設定群集的數量 k，然後找尋公式的極大值，以達到分群的最佳化之目的。</li>
<li>細節：<ul>
<li>假設：Assumes Euclidean space/distance</li>
<li>第一步：Start by picking <strong>$k$</strong>, the number of clusters. Initialize clusters by picking one point per cluster</li>
<li>第二步：For each point, place it in the cluster whose current centroid it is nearest</li>
<li>第三步：After all points are assigned, update the locations of centroids of the <strong>$k$</strong> clusters</li>
<li>第四步：Reassign all points to their closest centroid (Sometimes moves points between clusters)</li>
<li>重複步驟三四直到收斂（收斂：點不再在群間移動，且centroid也不再變動）</li>
</ul>
</li>
<li>問題：如何選擇要分成幾群？（即如何選擇 $k$?）<ul>
<li>Try different $k$, looking at the change in the average distance to centroid as $k$ increases.</li>
<li>Average falls rapidly until right $k$, then changes little</li>
<li>隨著 $k$ 增加，點到這 $k$ 個 centroid 的平均距離會跟著變動。而變動的幅度有大有小，變動幅度開始趨緩的那一點就是好 $k$<br><img src="https://hackpad-attachments.imgix.net/hackpad.com_5E9hkKwu42g_p.537642_1463802093357_undefined?fit=max&amp;w=882" alt=""></li>
</ul>
</li>
</ul>
<h2 id="BFR-Algorithm"><a href="#BFR-Algorithm" class="headerlink" title="BFR Algorithm"></a>BFR Algorithm</h2><ul>
<li>簡介：k-means 的延伸，適合處理大量資料</li>
<li>假設c群內的點相對於 centroid 的分佈是 normally distributed (in a Euclidean space) 而一個群在不同維度下的平均值和標準差可能不同，但是維度之間必須相互獨立</li>
<li>效率 vEfficient way to summarize clusters（只需要 $O(clusters)$ and not $O(data)$ 的記憶體）</li>
<li>前提：<ul>
<li>Points are read from disk one main-memory-full at a time</li>
<li>BFR 是用來處理非常大量的資料，因此資料會按照 set 的方式讀入。每個 set 裡的資料必須要保證可以在 main memory 裡，因為 main memory 中還要儲存其他必要訊息：DS, CS, and RS</li>
<li><strong>Most points from previous memory loads are summarized by simple statistics</strong></li>
</ul>
</li>
<li><p>細節：</p>
<ul>
<li>第一步：From the initial load we select the initial <strong>$k$</strong> centroids by some sensible approach:<ul>
<li>Take <strong>$k$</strong> random points</li>
<li>Take a small random sample and cluster optimally</li>
<li>Take a sample; pick a random point, and then </li>
<li><strong>$k–1$</strong> more points, each as far from the previously selected points as possible</li>
</ul>
</li>
<li><p>選完k之後分完群，會有三種類型的點：</p>
<ul>
<li><strong>Discard set</strong> (DS): Points close enough to a centroid to be summarized (summarized 不再儲存)<ul>
<li>The number of points, $N$ 所有點的數目</li>
<li>The vector <em>SUM</em>, whose $i$th component is the sum of the coordinates of the points in the $i$th dimension 所有點在每一維的向量之和，即 $SUM[i]$ 表示第 $i$ 維上的向量和</li>
<li>The vector $SUMSQ$: $i$th component = sum of squares of coordinates in $i$th dimension 所有點在每一維的向量的平方和</li>
<li>因此，如果資料是 $d$ 維的話，透過此表示法可以用 $2d+1$ 個值来表示一個 DS 或 CS</li>
<li>centroid 可以表示成 $SUM_i / N$，其中 $SUM_i = i$th component of $SUM$</li>
<li>DS 中 dimension $i$ 的 $Variance ＝ (SUMSQ_i / N) - (SUM_i / N)^{2}$</li>
</ul>
</li>
<li><strong>Compression set</strong> (CS): Groups of points that are close together but not close to any existing centroid. These points are summarized, but not assigned to a cluster (summarized 不再儲存)</li>
<li><strong>Retained set</strong> (RS): Isolated points waiting to be assigned to a compression set<br><img src="https://hackpad-attachments.imgix.net/hackpad.com_5E9hkKwu42g_p.537642_1463803595266_undefined?fit=max&amp;w=882" alt=""></li>
</ul>
</li>
<li><p>第二步：找到所有 <strong>sufficiently close</strong>（充分接近）某個群的 centroid 的點加入到該群中 These points are so close to the centroid that they can be summarized and then discarded</p>
</li>
<li>第三步：剩下的點以及 RS 可以使用任意main-memory clustering algorithm來分群，將分出來的 clusters 加入 CS; 獨立的點（outlying points）加入 RS</li>
<li>第四步：加入新的點後，DS 可以透過 N-SUM-SUMSQ 地表示法，直接加上這些新加入的點的 $N_{S}$, $SUM_{S}$, $SUMSQ_{S}$ 值即可。</li>
<li>第五步：透過第三步，有了新的 CS，它們和上一個 set 的資料處理後留下的 CS 和 RS 中間可能距離很近，是可以合併的。因此這一步就是將這些在 CS 中新舊的 cluster 和以前的 RS 進行合併。</li>
<li>第六步：如果這是最後一組資料，那麼就將這些 RS 的點和 CS 分配到距離最近的 cluster 中，否則繼續保留 RS 和 CS，等待和下一組資料一起處理。</li>
</ul>
</li>
<li>關於 BFR 的幾個問題<ul>
<li>How do we decide if a point is <strong>“close enough”</strong> to a cluster that we will add the point to that cluster? 多近才叫近呢？<ul>
<li>We need a way to decide whether to put a new point into a cluster (and discard)</li>
<li>兩種方法<ol>
<li>The <strong>Mahalanobis distance</strong> is less than a threshold 計算每個新的點和 centroid 的 MD<ul>
<li>Normalized Euclidean distance from centroid</li>
<li>For point $(x_{1}, …, x_{d})$ and centroid $(c_{1}, …, c_{d})$,<br><img src="https://hackpad-attachments.imgix.net/hackpad.com_5E9hkKwu42g_p.537642_1463841320539_undefined?fit=max&amp;w=882" alt=""></li>
<li>If clusters are normally distributed in $d$ dimensions, then after transformation, one standard deviation <strong>$= \sqrt{d}$</strong></li>
<li>Accept a point for a cluster if its M.D. is &lt; some threshold 點的MD小於門檻值則接受它（算它距離夠近）</li>
</ul>
</li>
<li>High likelihood of the point belonging to currently nearest centroid</li>
</ol>
</li>
</ul>
</li>
<li>How do we decide whether two compressed sets (CS) deserve to be combined into one?<ul>
<li>計算任兩個要合併的subcluster的variance （利用 <strong>$N$</strong>, <strong>$SUM$</strong>, and <strong>$SUMSQ$</strong> 可以算得很快）</li>
<li>當 var 小於某個門檻值，則合併</li>
<li>其他方法：Treat dimensions differently, consider density</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Slides"><a href="#Slides" class="headerlink" title="Slides"></a>Slides</h2><ul>
<li>from CS246 Mining Massive Data Sets, Stanford University</li>
<li><a href="http://web.stanford.edu/class/cs246/slides/clustering.pdf" target="_blank" rel="external">http://web.stanford.edu/class/cs246/slides/clustering.pdf</a></li>
</ul>
</div><div class="tags"></div><div class="post-nav"><a href="/blog/2016/05/24/mda-ch8-advertising-on-the-web/" class="pre">[Massive Data Analysis] Advertising on the Web</a><a href="/blog/2016/05/23/mda-ch6-frequent-itemsets/" class="next">[Massive Data Analysis] Frequent itemsets</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://shannywu.github.io/blog"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/blog/2020/04/19/airflow-kubernetes-pod-operator-cloud-composer/">[Airflow] Using the KubernetesPodOperator on Cloud Composer</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2020/04/03/airflow-scheduling/">[Airflow] Scheduling</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2019/02/16/spark-job-on-amazon-emr/">[Spark] Run Spark Job on Amazon EMR</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/11/22/python-nltk-tools/">[Python] NLTK 工具整理</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/07/26/windows-flask-mod-wsgi-apache-on-windows/">Flask + mod_wsgi + Apache on Windows</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/07/17/python-nltk-wordnet/">[Python] NLTK and WordNet</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/06/16/tools-tmux-a-terminal-multiplexer/">tmux - A terminal multiplexer</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/05/24/mda-ch9-recommendation-systems/">[Massive Data Analysis] Recommendation Systems</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/05/24/mda-ch8-advertising-on-the-web/">[Massive Data Analysis] Advertising on the Web</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/05/23/mda-chapter-7-clustering/">[Massive Data Analysis] Clustering</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Data-Engineering/">Data Engineering</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Lecture-Notes/">Lecture Notes</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Miscellaneous/">Miscellaneous</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/blog/tags/flask/" style="font-size: 15px;">flask</a> <a href="/blog/tags/nltk/" style="font-size: 15px;">nltk</a> <a href="/blog/tags/re/" style="font-size: 15px;">re</a> <a href="/blog/tags/standford-parser/" style="font-size: 15px;">standford parser</a> <a href="/blog/tags/wordnet/" style="font-size: 15px;">wordnet</a> <a href="/blog/tags/linux/" style="font-size: 15px;">linux</a> <a href="/blog/tags/sort/" style="font-size: 15px;">sort</a> <a href="/blog/tags/grep/" style="font-size: 15px;">grep</a> <a href="/blog/tags/zgrep/" style="font-size: 15px;">zgrep</a> <a href="/blog/tags/edit-distance/" style="font-size: 15px;">edit distance</a> <a href="/blog/tags/spelling-correction/" style="font-size: 15px;">spelling correction</a> <a href="/blog/tags/collocation/" style="font-size: 15px;">collocation</a> <a href="/blog/tags/Linggle/" style="font-size: 15px;">Linggle</a> <a href="/blog/tags/map-reduce/" style="font-size: 15px;">map reduce</a> <a href="/blog/tags/pattern-grammar/" style="font-size: 15px;">pattern grammar</a> <a href="/blog/tags/WriteAhead/" style="font-size: 15px;">WriteAhead</a> <a href="/blog/tags/filter/" style="font-size: 15px;">filter</a> <a href="/blog/tags/lambda/" style="font-size: 15px;">lambda</a> <a href="/blog/tags/map/" style="font-size: 15px;">map</a> <a href="/blog/tags/match/" style="font-size: 15px;">match</a> <a href="/blog/tags/search/" style="font-size: 15px;">search</a> <a href="/blog/tags/reduce/" style="font-size: 15px;">reduce</a> <a href="/blog/tags/tmux/" style="font-size: 15px;">tmux</a> <a href="/blog/tags/apache/" style="font-size: 15px;">apache</a> <a href="/blog/tags/mod-wsgi/" style="font-size: 15px;">mod_wsgi</a> <a href="/blog/tags/windows/" style="font-size: 15px;">windows</a> <a href="/blog/tags/Airflow/" style="font-size: 15px;">Airflow</a> <a href="/blog/tags/Data-Engineering/" style="font-size: 15px;">Data Engineering</a> <a href="/blog/tags/Cloud-Composer/" style="font-size: 15px;">Cloud Composer</a> <a href="/blog/tags/GCP/" style="font-size: 15px;">GCP</a> <a href="/blog/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/blog/tags/EMR/" style="font-size: 15px;">EMR</a> <a href="/blog/tags/AWS/" style="font-size: 15px;">AWS</a></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/blog/." rel="nofollow">learning notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/blog/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/blog/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/blog/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/blog/js/smartresize.js?v=0.0.0"></script></div></body></html>