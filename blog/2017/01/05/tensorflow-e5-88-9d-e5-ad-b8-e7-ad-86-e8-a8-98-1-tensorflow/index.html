<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Tensorflow 初學筆記 (1) Tensorflow 簡介 | learning notes</title><link rel="stylesheet" type="text/css" href="/blog/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/blog/favicon.ico"><link rel="apple-touch-icon" href="/blog/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/blog/apple-touch-icon.png"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-68820773-2','auto');ga('send','pageview');</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Tensorflow 初學筆記 (1) Tensorflow 簡介</h1><a id="logo" href="/blog/.">learning notes</a><p class="description"></p></div><div id="nav-menu"><a href="/blog/." class="current"><i class="fa fa-home"> Home</i></a><a href="/blog/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/blog/about/"><i class="fa fa-user"> About</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Tensorflow 初學筆記 (1) Tensorflow 簡介</h1><div class="post-meta">Jan 5, 2017<span> | </span><span class="category"><a href="/blog/categories/Machine-Learning/">Machine Learning</a></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#什麼是-Tensorflow"><span class="toc-number">1.</span> <span class="toc-text">什麼是 Tensorflow?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensorflow-的架構"><span class="toc-number">2.</span> <span class="toc-text">Tensorflow 的架構</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#簡單的程式碼"><span class="toc-number">3.</span> <span class="toc-text">簡單的程式碼</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#參考連結"><span class="toc-number">4.</span> <span class="toc-text">參考連結</span></a></li></ol></div></div><div class="post-content"><p>這系列紀錄了學習 Google Tensorflow 的歷程和一些小心得，主要是參考官方文件。<br>先從簡介開始吧。</p>
<p><br><br><a id="more"></a></p>
<h2 id="什麼是-Tensorflow"><a href="#什麼是-Tensorflow" class="headerlink" title="什麼是 Tensorflow?"></a>什麼是 Tensorflow?</h2><p>TensorFlow 是 Google 繼 2011 年開發了 DistBelief 之後，透過使用資料流 (flow) 圖像，來進行數值演算的新一代開源機器學習工具。這個機器學習工具的基礎設計，主要透過圖學裡的「節點」來表達數學運算，「邊」 來表示「節點」間的多維度資料陣列 (tensors，張量)，因此命名做 TensorFlow。TensorFlow 主要由 Google 機器智慧研究室與 Google 大腦研究組 (Google Brain Team) 的學者與工程師所開發，容許開發者自由配置運算環境來做深度神經網絡研究，但也足以支持普通環境所需要的服務（例如透過影片進行圖像辨識）。</p>
<p>[youtube <a href="https://www.youtube.com/watch?v=oZikw5k_2FM&amp;w=560&amp;h=315" target="_blank" rel="external">https://www.youtube.com/watch?v=oZikw5k_2FM&amp;w=560&amp;h=315</a>]</p>
<h2 id="Tensorflow-的架構"><a href="#Tensorflow-的架構" class="headerlink" title="Tensorflow 的架構"></a>Tensorflow 的架構</h2><p>Tensorflow 首先要定義類神經網路的架構，然後再把資料放進架構中去做運算。如下圖：</p>
<p><img src="https://codingdiarysite.files.wordpress.com/2017/02/tensors_flowing.gif" alt="tensors_flowing"></p>
<p>Tensorflow 是透過圖學的概念來運作，一開始要先建立一個 data flow，再將資料放進去。其中節點（Nodes）在圖中表示數學運算，邊（edges）則表示節點間相互聯繫的多維度資料陣列，也就是張量（tensor）。訓練模型時，tensor 會不斷從 data flow graph 中的一個節點流（flow）到另一個節點，這就是 Tensorflow 名稱的由來。</p>
<h2 id="簡單的程式碼"><a href="#簡單的程式碼" class="headerlink" title="簡單的程式碼"></a>簡單的程式碼</h2><p>這段來看看要怎麼用簡單的<a href="https://www.tensorflow.org/get_started/" target="_blank" rel="external">程式碼</a>來建立一個 Tensorflow。</p>
<p>首先，將 tensorflow 和 numpy import 進來，numpy 是用來產生簡單的資料。</p>
<figure class="highlight elm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"># <span class="type">Create</span> <span class="number">100</span> phony x, y data points <span class="keyword">in</span> <span class="type">NumPy</span>, y = x * <span class="number">0.1</span> + <span class="number">0.3</span></div><div class="line"><span class="title">x_data</span> = np.random.rand(<span class="number">100</span>).<span class="keyword">as</span><span class="keyword">type</span>(np.float32)</div><div class="line"><span class="title">y_data</span> = x_data * <span class="number">0.1</span> + <span class="number">0.3</span></div></pre></td></tr></table></figure>
<p>接著，用 $tf.Variable$ 來將例描述 $y$ 的參數 $W$(weights) 和 $b$(bias)。透過前面的 $y_data = x_data * 0.1 + 0.3$，我們都知道 $W$ 會是 $0.1$，而 $b$ 是 $0.3$。Tensorflow 的工作就是一步步學習、找出答案，最後將 $W$ 學成 $0.1$，$b$ 學成 $0.3$。</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># Try <span class="keyword">to</span> <span class="keyword">find</span> <span class="built_in">values</span> <span class="keyword">for</span> W <span class="built_in">and</span> <span class="keyword">b</span> that compute y_data = W * x_data + <span class="keyword">b</span></div><div class="line"># (We know that W should <span class="keyword">be</span> <span class="number">0.1</span> <span class="built_in">and</span> <span class="keyword">b</span> <span class="number">0.3</span>, </div><div class="line"># but TensorFlow will figure that out <span class="keyword">for</span> us.)</div><div class="line"></div><div class="line">W = <span class="keyword">tf</span>.Variable(<span class="keyword">tf</span>.random_uniform([<span class="number">1</span>], -<span class="number">1.0</span>, <span class="number">1.0</span>))  # <span class="number">1</span> dimension, <span class="built_in">range</span> from -<span class="number">1.0</span> <span class="keyword">to</span> <span class="number">1.0</span></div><div class="line"><span class="keyword">b</span> = <span class="keyword">tf</span>.Variable(<span class="keyword">tf</span>.zeros([<span class="number">1</span>]))  # <span class="number">1</span> dimension, <span class="keyword">set</span> <span class="keyword">to</span> zero</div><div class="line"><span class="keyword">y</span> = W * x_data + <span class="keyword">b</span></div></pre></td></tr></table></figure>
<p>接著計算 $y$ 和 $y_data$ 的誤差，即 $y$ 和實際 $y_data$ 的差距。這裡的 optimizer 利用 Gradient Descent 來進行 Back Propagation。然後利用這個 optimizer 來減少誤差，提升參數的準確度，而目標就是將誤差最小化。</p>
<figure class="highlight makefile"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Minimize the mean squared errors.</span></div><div class="line">loss = tf.reduce_mean(tf.square(y - y_data))</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(0.5)  <span class="comment">#learning rate = 0.5</span></div><div class="line">train = optimizer.minimize(loss)</div></pre></td></tr></table></figure>
<p>目前為止，只是將 Tensorflow 的架構建起來，在使用這個架構之前，必須先將之前定義的所有 Variable 初始化。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Before starting, initialize the variables.  We will 'run' this first.</span></div><div class="line"><span class="attr">init</span> = tf.global_variables_initializer()</div></pre></td></tr></table></figure>
<p>真正開始之前，還有一步，就是建立 Session，利用 Session 來執行剛剛定義的 init 初始化的步驟。</p>
<figure class="highlight makefile"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Launch the graph.</span></div><div class="line">sess = tf.Session()</div><div class="line">sess.run(init)</div></pre></td></tr></table></figure>
<p>現在，就讓這個神經網路開始一步步的訓練，也就是上面定義的 train。這個訓練一樣是用 Session 來進行，Session 每 run 一次 train，optimizer 就會去減少 $y$ 和 $y_data$ 的誤差，漸漸地這個誤差會越來越小，而理論上預測的準確性也會越來越高。</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Fit the line.</span></div><div class="line"><span class="keyword">for</span> <span class="keyword">step</span> <span class="keyword">in</span> range(201):  # train 200 times</div><div class="line">    sess.<span class="builtin-name">run</span>(train)</div><div class="line">    <span class="keyword">if</span> <span class="keyword">step</span> % 20 == 0:  # <span class="builtin-name">print</span> Weight <span class="keyword">and</span> bias every 20 steps</div><div class="line">        <span class="builtin-name">print</span>(<span class="keyword">step</span>, sess.<span class="builtin-name">run</span>(W), sess.<span class="builtin-name">run</span>(b))</div><div class="line"></div><div class="line"><span class="comment"># Learns best fit is W: [0.1], b: [0.3]</span></div></pre></td></tr></table></figure>
<p>這是完整的程式碼執行結果：</p>
<p><pre>0 [ 0.26253623] [ 0.29924595]<br>20 [ 0.13903731] [ 0.27772433]<br>40 [ 0.1123959] [ 0.29292658]<br>60 [ 0.10393622] [ 0.2977539]<br>80 [ 0.10124992] [ 0.29928678]<br>100 [ 0.10039689] [ 0.29977354]<br>120 [ 0.10012604] [ 0.2999281]<br>140 [ 0.10004003] [ 0.29997718]<br>160 [ 0.10001272] [ 0.29999277]<br>180 [ 0.10000405] [ 0.29999772]<br>200 [ 0.10000129] [ 0.29999927]<br></pre><br>可以看到，Weight 和 bias 經過學習之後，會慢慢地逼近 0.1 和 0.3。而這就是整個 Tensorflow 的基礎架構和基本用法。</p>
<h2 id="參考連結"><a href="#參考連結" class="headerlink" title="參考連結"></a>參考連結</h2><ul>
<li><a href="https://www.bnext.com.tw/article/37925/BN-2015-11-10-184646-40" target="_blank" rel="external">https://www.bnext.com.tw/article/37925/BN-2015-11-10-184646-40</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="external">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/</a></li>
</ul>
</div><div class="tags"></div><div class="post-nav"><a href="/blog/2017/01/06/tensorflow-e5-88-9d-e5-ad-b8-e7-ad-86-e8-a8-98-2-session/" class="pre">Tensorflow 初學筆記 (2) Session</a><a href="/blog/2016/07/26/windows-flask-mod-wsgi-apache-on-windows/" class="next">Flask + mod_wsgi + Apache on Windows</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://shannywu.github.io/blog"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/blog/2017/01/10/tensorflow-e5-88-9d-e5-ad-b8-e7-ad-86-e8-a8-98-3-variable/">Tensorflow 初學筆記 (3) Variable</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2017/01/06/tensorflow-e5-88-9d-e5-ad-b8-e7-ad-86-e8-a8-98-2-session/">Tensorflow 初學筆記 (2) Session</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2017/01/05/tensorflow-e5-88-9d-e5-ad-b8-e7-ad-86-e8-a8-98-1-tensorflow/">Tensorflow 初學筆記 (1) Tensorflow 簡介</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/07/26/windows-flask-mod-wsgi-apache-on-windows/">Flask + mod_wsgi + Apache on Windows</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/06/16/tools-tmux-a-terminal-multiplexer/">tmux - A terminal multiplexer</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/05/24/mda-ch9-recommendation-systems/">[Massive Data Analysis] Recommendation Systems</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/05/24/mda-ch8-advertising-on-the-web/">[Massive Data Analysis] Advertising on the Web</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/05/23/mda-chapter-7-clustering/">[Massive Data Analysis] Clustering</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/05/23/mda-ch6-frequent-itemsets/">[Massive Data Analysis] Frequent itemsets</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2016/05/22/mda-ch5-link-analysis/">[Massive Data Analysis] Link Analysis</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Lecture-Notes/">Lecture Notes</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Miscellaneous/">Miscellaneous</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Python/">Python</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/blog/tags/re/" style="font-size: 15px;">re</a> <a href="/blog/tags/flask/" style="font-size: 15px;">flask</a> <a href="/blog/tags/grep/" style="font-size: 15px;">grep</a> <a href="/blog/tags/linux/" style="font-size: 15px;">linux</a> <a href="/blog/tags/zgrep/" style="font-size: 15px;">zgrep</a> <a href="/blog/tags/sort/" style="font-size: 15px;">sort</a> <a href="/blog/tags/edit-distance/" style="font-size: 15px;">edit distance</a> <a href="/blog/tags/spelling-correction/" style="font-size: 15px;">spelling correction</a> <a href="/blog/tags/collocation/" style="font-size: 15px;">collocation</a> <a href="/blog/tags/pattern-grammar/" style="font-size: 15px;">pattern grammar</a> <a href="/blog/tags/WriteAhead/" style="font-size: 15px;">WriteAhead</a> <a href="/blog/tags/Linggle/" style="font-size: 15px;">Linggle</a> <a href="/blog/tags/map-reduce/" style="font-size: 15px;">map reduce</a> <a href="/blog/tags/lambda/" style="font-size: 15px;">lambda</a> <a href="/blog/tags/map/" style="font-size: 15px;">map</a> <a href="/blog/tags/filter/" style="font-size: 15px;">filter</a> <a href="/blog/tags/match/" style="font-size: 15px;">match</a> <a href="/blog/tags/search/" style="font-size: 15px;">search</a> <a href="/blog/tags/reduce/" style="font-size: 15px;">reduce</a> <a href="/blog/tags/tmux/" style="font-size: 15px;">tmux</a> <a href="/blog/tags/apache/" style="font-size: 15px;">apache</a> <a href="/blog/tags/mod-wsgi/" style="font-size: 15px;">mod_wsgi</a> <a href="/blog/tags/windows/" style="font-size: 15px;">windows</a></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2017 <a href="/blog/." rel="nofollow">learning notes.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/blog/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/blog/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/blog/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/blog/js/smartresize.js?v=0.0.0"></script></div></body></html>